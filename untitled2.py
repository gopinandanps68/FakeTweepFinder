# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j6-dXYBd5PmsHz8UYRd8LH5Evl_h_UFE
"""
import numpy as np
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import f1_score
import warnings
import joblib

warnings.filterwarnings('ignore')
nltk.download('stopwords')

# Load the dataset
data_train = pd.read_csv("unique_tweets_dataset.csv")
print("Data shape:", data_train.shape)

# Drop unnecessary columns
#data_train = data_train.drop(['id'], axis=1)

# Ensure text column is valid
data_train = data_train.dropna(subset=['text'])
data_train['text'] = data_train['text'].astype(str)

# Preprocess the text data
def preprocess_text(text):
    pstem = PorterStemmer()
    text = re.sub("[^a-zA-Z]", " ", text)
    text = text.lower()
    text = text.split()
    text = [pstem.stem(word) for word in text if word not in set(stopwords.words('english'))]
    return " ".join(text)

data_train['cleaned_text'] = data_train['text'].apply(preprocess_text)
print("Text preprocessing completed.")

# Create TF-IDF representation
tfidfVec = TfidfVectorizer(max_features=5000)
X = tfidfVec.fit_transform(data_train['cleaned_text']).toarray()
y = data_train['target']
print("Vectorization completed. Feature size:", X.shape[1])

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=55, shuffle=True)
print("Data splitting completed.")

# Define models
models = {
    "DecisionTree": DecisionTreeClassifier(criterion='entropy', random_state=55),
    "GradientBoosting": GradientBoostingClassifier(loss='log_loss', learning_rate=0.01, n_estimators=100, max_depth=3, random_state=55),
    "KNeighbors": KNeighborsClassifier(n_neighbors=7, weights='distance', algorithm='brute'),
    "LogisticRegression": LogisticRegression(penalty='l2', solver='saga', random_state=55),
    "SGDClassifier": SGDClassifier(loss='hinge', penalty='l1', learning_rate='optimal', max_iter=100, random_state=55),
    "SVC": SVC(kernel='linear', C=2, random_state=55, max_iter=10000),
    "BernoulliNB": BernoulliNB(alpha=0.1),
    "GaussianNB": GaussianNB(),
    "MultinomialNB": MultinomialNB(alpha=0.1)
}

# Train and evaluate models
for name, model in models.items():
    model.fit(X_train, y_train)
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    y_pred = model.predict(X_test)
    f1 = f1_score(y_test, y_pred)
    print(f"{name}: Train Score = {train_score:.4f}, Test Score = {test_score:.4f}, F1 Score = {f1:.4f}")

# Save the vectorizer and one model as examples
joblib.dump(tfidfVec, 'tfidf_vectorizer.pkl')
joblib.dump(models['LogisticRegression'], 'logistic_regression_model.pkl')

print("Models and vectorizer saved successfully.")
